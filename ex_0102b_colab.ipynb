{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ex_0102b_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soohyoen/artificial-intelligence/blob/main/ex_0102b_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ec833f"
      },
      "source": [
        "### 나이브 베이즈 분류기 - 영화 리뷰."
      ],
      "id": "a5ec833f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cebe945"
      },
      "source": [
        "**<span style=\"color:blue\">목표</span>**: 베이즈 정리를 적용하여 자연어 분류 모형을 만들어 본다."
      ],
      "id": "4cebe945"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ef8a1d"
      },
      "source": [
        "<br>\n",
        "1. 베이즈 정리. <br>\n",
        "\n",
        " $P(A| W_1, W_2, W_3,...) = {P(W_1, W_2, W_3,...|A) \\cdot P(A) \\over P(W_1, W_2, W_3,....)}$ <br>\n",
        " $P(B| W_1, W_2, W_3,...) = {P(W_1, W_2, W_3,...|B) \\cdot P(B) \\over P(W_1, W_2, W_3,....)}$\n",
        "\n",
        "<br>\n",
        "2. 동일한 분모를 무시하면 다음 비례관계가 성립된다. <br>\n",
        "\n",
        " $P(A| W_1, W_2, W_3, \\ldots) \\sim P(W_1, W_2, W_3,...|A) \\cdot P(A) $ <br>\n",
        " $P(B| W_1, W_2, W_3, \\ldots) \\sim P(W_1, W_2, W_3,...|B) \\cdot P(B) $\n",
        " \n",
        "<br>\n",
        "3. 독립적인 단어 분포를 전제하면 다음과 같이 분해할 수 있다. <br>\n",
        "\n",
        " $P(A| W_1, W_2, W_3, \\ldots) \\sim  P(W_1|A)\\cdot P(W_2|A)\\cdot P(W_3|A) \\cdots P(A) $ <br>\n",
        " $P(B| W_1, W_2, W_3, \\ldots) \\sim P(W_1|B)\\cdot P(W_2|B)\\cdot P(W_3|B) \\cdots P(B) $\n",
        "\n",
        "<br>\n",
        "4. 이제는 로그함수를 적용해 본다. <br>\n",
        "\n",
        "$Log_A \\sim  Log(P(W_1|A)) +  Log(P(W_2|A)) + Log(P(W_3|A)) + \\ldots + Log(P(A)) $ <br>\n",
        "$Log_B \\sim  Log(P(W_1|B)) +  Log(P(W_2|B)) + Log(P(W_3|B)) + \\ldots + Log(P(B)) $\n",
        "<br>\n",
        "\n",
        "**<span style=\"color:blue\">결론</span>**:  $Log_A$와 $Log_B$를 비교해서 큰 쪽으로 인식!"
      ],
      "id": "82ef8a1d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "395f8367"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.datasets import load_files"
      ],
      "id": "395f8367",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3170ba18"
      },
      "source": [
        "#### 1. 학습 자료를 읽어와서 전처리한다:"
      ],
      "id": "3170ba18"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsnVZXYCrFGn"
      },
      "source": [
        "# 구글 드라이브 마운트.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # 절차를 따라서 한다."
      ],
      "id": "FsnVZXYCrFGn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnwDEHWprF3m"
      },
      "source": [
        "# 경로 이동.\n",
        "%cd \"/content/drive/MyDrive/GwangjuAI/modelling/notebook\""
      ],
      "id": "VnwDEHWprF3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fa7dd9b"
      },
      "source": [
        "# 데이터 폴더 설정 후 읽어온다.\n",
        "reviews = load_files('../data/txt_sentoken/')\n",
        "my_docs, y = reviews.data, reviews.target"
      ],
      "id": "1fa7dd9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a7c606f"
      },
      "source": [
        "# 0 유형의 리뷰를 가져온다. \n",
        "my_docs_0 = list(pd.Series(my_docs)[pd.Series(y) == 0])\n",
        "\n",
        "# 1 유형의 리뷰를 가져온다. \n",
        "my_docs_1 = list(pd.Series(my_docs)[pd.Series(y) == 1])"
      ],
      "id": "4a7c606f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92a43ac7"
      },
      "source": [
        "print(f\"0 유형의 수 : {len(my_docs_0)}\")\n",
        "print(f\"1 유형의 수 : {len(my_docs_1)}\")"
      ],
      "id": "92a43ac7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "601e37fe"
      },
      "source": [
        "#출력해 본다.\n",
        "# Label 0 = negative 이며 Label 1 = positive 임을 확인할 수 있다.\n",
        "#print(my_docs_0[0])\n",
        "#print(my_docs_1[0])"
      ],
      "id": "601e37fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18e3da1d"
      },
      "source": [
        "# 학습용과 테스트 용으로 분리해 놓는다.\n",
        "n_train = 700\n",
        "n_test = 1000 - n_train\n",
        "my_docs_train_0 = my_docs_0[:n_train]\n",
        "my_docs_train_1 = my_docs_1[:n_train]\n",
        "my_docs_test = my_docs_0[n_train:] + my_docs_1[n_train:]\n",
        "Y_test = [0]*n_test + [1]*n_test"
      ],
      "id": "18e3da1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68005f4c"
      },
      "source": [
        "# 전처리 해서 단어별 도수표를 만들어주는 함수.\n",
        "def preprocessor(a_doc):\n",
        "    freq_dict = {}\n",
        "    for a_line in a_doc:\n",
        "        a_line = str(a_line)                  # 단순한 문자열로 변환.\n",
        "        a_line = a_line.lower()               # 소문자화.\n",
        "        a_line = re.sub(r\"\\W\",\" \",a_line)     # 특수문자 제거.\n",
        "        a_line = re.sub(r\"\\d\", \" \", a_line)   # 숫자 제거.\n",
        "        a_line = re.sub(\"a|the|and|or|because|at\", \" \",a_line)  #  불용어 제거.\n",
        "        a_line = a_line.split()               # 분절.\n",
        "        for a_word in a_line:\n",
        "            if len(a_word) > 3:               # 길이가 최소 조건을 충족하는 단어만 사용.            \n",
        "                if a_word in freq_dict:\n",
        "                    freq_dict[a_word] += 1    # 카운트 누적.\n",
        "                else:\n",
        "                    freq_dict[a_word] = 2     # 처음 발견. 기본값 1 + 누적 1.\n",
        "    return freq_dict"
      ],
      "id": "68005f4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436c4253"
      },
      "source": [
        "#### 2. 학습 모형을 준비해 둔다:"
      ],
      "id": "436c4253"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acb1bc2"
      },
      "source": [
        "# Series로 변환.\n",
        "freq_0 = pd.Series(preprocessor(my_docs_train_0)).sort_values(ascending=False)\n",
        "freq_1 = pd.Series(preprocessor(my_docs_train_1)).sort_values(ascending=False) "
      ],
      "id": "1acb1bc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78fcd1a6"
      },
      "source": [
        "# Vocabulary 사전 크기 확인.\n",
        "print(\"Size of 0: {}\".format( len(freq_0)))\n",
        "print(\"Size of 1: {}\".format( len(freq_1)))"
      ],
      "id": "78fcd1a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08c8716f"
      },
      "source": [
        "# Vocabulary 수 맞춤.\n",
        "n_voca = 15000\n",
        "freq_0 = freq_0.iloc[:n_voca]\n",
        "freq_1 = freq_1.iloc[:n_voca]"
      ],
      "id": "08c8716f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93e0a3d0"
      },
      "source": [
        "# 로그 확률로 변환.\n",
        "freq_0_sum = freq_0.sum()\n",
        "freq_1_sum = freq_1.sum()\n",
        "log_prob_0 = dict(np.log(freq_0/freq_0_sum))    # Log(P(W|0))\n",
        "log_prob_1 = dict(np.log(freq_1/freq_1_sum))    # Log(P(W|1))"
      ],
      "id": "93e0a3d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5536f16"
      },
      "source": [
        "#### 3. 예측을 실시한다:"
      ],
      "id": "f5536f16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99f65107"
      },
      "source": [
        "Y_pred = []\n",
        "for a_sentence in my_docs_test:\n",
        "    log_prob_sum_0 = 0.\n",
        "    log_prob_sum_1 = 0.\n",
        "    a_sent_preprocessed = preprocessor([a_sentence])\n",
        "    for a_word, a_freq in a_sent_preprocessed.items():\n",
        "        if a_word in log_prob_0:\n",
        "            log_prob_sum_0 += log_prob_0[a_word]*a_freq\n",
        "        else:\n",
        "            log_prob_sum_0 += np.log(1.0/freq_0_sum)*a_freq\n",
        "            \n",
        "        if a_word in log_prob_1:\n",
        "            log_prob_sum_1 += log_prob_1[a_word]*a_freq\n",
        "        else:\n",
        "            log_prob_sum_1 += np.log(1.0/freq_1_sum)*a_freq\n",
        "            \n",
        "    if (log_prob_sum_0 > log_prob_sum_1):\n",
        "        Y_pred.append(0)\n",
        "    else:\n",
        "        Y_pred.append(1)"
      ],
      "id": "99f65107",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ad5b9d7"
      },
      "source": [
        "# 정확도 계산.\n",
        "correct = pd.Series([ x == y for (x,y) in zip(Y_pred, Y_test) ])\n",
        "print(\"Accuracy : {:.3f}\".format(correct.mean()))"
      ],
      "id": "2ad5b9d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9f71adc"
      },
      "source": [
        ""
      ],
      "id": "e9f71adc",
      "execution_count": null,
      "outputs": []
    }
  ]
}