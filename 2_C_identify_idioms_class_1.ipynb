{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_C_identify_idioms_class_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soohyoen/artificial-intelligence/blob/main/2_C_identify_idioms_class_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAggOWDIgWz"
      },
      "source": [
        "# 2-C Identify Idioms\n",
        "- author: Eu-Bin KIM @likelion\n",
        "- 14th of August 2021\n",
        "- tlrndk123@gmail.com\n",
        "\n",
        "\n",
        "## To-do's\n",
        "- `build_patterns`\n",
        "- `build_patterns_list_comp`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mhLKfkPtGZw",
        "outputId": "5c2bb26b-7dd7-4bce-dfd9-9abdec3e2495"
      },
      "source": [
        "# spacy 라이브러리 설치\n",
        "# https://spacy.io/\n",
        "!pip3 install spacy\n",
        "# 사전훈련된 nlp 모델 다운로드 \n",
        "# spacy - 예측기반 모델. \"가중치\" -> 따로 다운로드를 받아야 한다.\n",
        "# https://spacy.io/models/en#en_core_web_sm\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_1sv9CQIT9_"
      },
      "source": [
        "# --- the libraries needed --- #\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from typing import List, Dict, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXRQ0ac0tO-r"
      },
      "source": [
        "# --- global constants & variables --- #\n",
        "# 이번숙제의 목표는, 여려 용례로부터 다음 두 관용구를 검출해내는 것입니다.\n",
        "IDIOMS = [\n",
        "          # 경제적으로 독립하다, 라는 뜻의 관용구.\n",
        "          # https://idioms.thefreedictionary.com/stand+on+one%27s+own+feet\n",
        "          \"stand on one's own feet\",\n",
        "          # \"눈을 뜨다\"; 시야를 넓히다. \n",
        "          # https://www.merriam-webster.com/dictionary/open%20one%27s%20eyes\n",
        "          \"open one's eyes\"\n",
        "]\n",
        "\n",
        "# 다음 용례로부터 해당 관용구를 검출해내면 됩니다:\n",
        "# (입력, 정답) = (용례, 관용구)로 설정하도록 하겠습니다. 그래서 list of tuples 입니다.                            \n",
        "BATCH: List[Tuple[str , str]] = [\n",
        "    # --- stand on one's own feet의 용례 --- #\n",
        "    (\"if you don't want to do the chores, move out and stand on your own feet!\", IDIOMS[0]), # one's -> your\n",
        "    (\"It's difficult for students to stand on their own feet without help from their parents\", IDIOMS[0]), # one's -> their\n",
        "    (\"I've been standing on my own feet since I was sixteen years old\",  IDIOMS[0]), # stand -> standing, one's -> my\n",
        "    # --- open one's eyes의 용례 --- #\n",
        "    (\"the letter finally opened my eyes to the truth\", IDIOMS[1]),  # open -> opended, one's -> my\n",
        "    (\"The documentary really opened her eyes to the conditions in that country\", IDIOMS[1]), # open -> opended, one's -> her\n",
        "]\n",
        "# natural language pipeline (nlp) 로드하기\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# stand on one's own feet으로부터, 소유격이 들어가야 하는 부분은 one's 로 표기될 것임을 알 수 있습니다.\n",
        "# 나중에 키워드로 사용하기 위해 여기에 미리 상수로 정의해둘게요.\n",
        "PRP_PLACEHOLDER = \"one's\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VW5tFmJNJqw"
      },
      "source": [
        "**Question: `spacy` 의 `nlp`는 뭐죠? 이걸로 무엇을 할 수 있나요?**\n",
        "\n",
        "> Answer: 토큰화, 표제어 추출, 품사 추출 등의 작업을 매우 빠르게, 한번에 처리할 수 있는 SpaCy를 대표하는 파이프라인 입니다.\n",
        "- `token.text`: 토큰화된 결과 확인\n",
        "- `token.lemma_`: 표제어 추출 결과 확인\n",
        "- `token.pos_`: coarse 품사 추출 결과 확인\n",
        "- `token.tag_`: fine-grained 품사 추출 결과 확인\n",
        "- `token.is_stop` : 불용어인지 아닌지?\n",
        "- `token.is_punct` : punctuation인지 아닌지?\n",
        "- 이외에도 더 많은 attributes를 접근할 수 있으며, 접근 가능한\n",
        " 모든 attributes의 리스트는 [이 문서](https://spacy.io/api/token#attributes)에서 확인가능합니다. \n",
        "- natural language pipeline에 대한 더 자세한 설명은 [이 문서](https://spacy.io/api#architecture-pipeline)를 참조하세요. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlrQCmwtu6lO",
        "outputId": "e46bca9a-209b-4e31-a8d2-b7f3da75eca0"
      },
      "source": [
        "# 한번 배치 속 첫번째 문서를 nlp로 처리해볼까요?\n",
        "sent = BATCH[0][0]  \n",
        "attrs = [\n",
        "           # (토큰, 추출된 표제어, 추출된 품사(coarse), 추출된 더 자세한 품사(fine-grained))\n",
        "           (token.text, token.lemma_, token.pos_, token.tag_, token.norm_)\n",
        "           for token in nlp(sent)\n",
        "           # nlp(sent) -> Doc -> 루프 -> Token -> token.text, token.lemma_,\n",
        "           # list comprehension의 filter를 정의하면 입맛대로 전처리가 가능함.\n",
        "          #  if not token.is_stop \n",
        "          #  if not token.is_punct\n",
        "          #  if token.pos_ == \"NOUN\"\n",
        "]\n",
        "for info in attrs:\n",
        "  print(info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('if', 'if', 'SCONJ', 'IN', 'if')\n",
            "('you', '-PRON-', 'PRON', 'PRP', 'you')\n",
            "('do', 'do', 'AUX', 'VBP', 'do')\n",
            "(\"n't\", 'not', 'PART', 'RB', 'not')\n",
            "('want', 'want', 'VERB', 'VB', 'want')\n",
            "('to', 'to', 'PART', 'TO', 'to')\n",
            "('do', 'do', 'AUX', 'VB', 'do')\n",
            "('the', 'the', 'DET', 'DT', 'the')\n",
            "('chores', 'chore', 'NOUN', 'NNS', 'chores')\n",
            "(',', ',', 'PUNCT', ',', ',')\n",
            "('move', 'move', 'VERB', 'VB', 'move')\n",
            "('out', 'out', 'ADV', 'RB', 'out')\n",
            "('and', 'and', 'CCONJ', 'CC', 'and')\n",
            "('stand', 'stand', 'VERB', 'VB', 'stand')\n",
            "('on', 'on', 'ADP', 'IN', 'on')\n",
            "('your', '-PRON-', 'DET', 'PRP$', 'your')\n",
            "('own', 'own', 'ADJ', 'JJ', 'own')\n",
            "('feet', 'foot', 'NOUN', 'NNS', 'feet')\n",
            "('!', '!', 'PUNCT', '.', '!')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikTCgCDHMBvw"
      },
      "source": [
        "**Question:  `token.pos_`, `token.tag_`에 담겨 있는 기호의 의미가 무엇인가요?**\n",
        "\n",
        "> Answer: `spacy.explain()` 함수를 사용하면, 각 태그가 무엇을 뜻하는지 확인해볼수 있습니다.\n",
        "- `token.pos_` = coarse(일반적인) 품사: [Universal Part of Speech](https://universaldependencies.org/u/pos/) 기반\n",
        "- `token.tag_` = fine-grained(구체적인) 품사: [Penn Tree bank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) 기반\n",
        "- [spacy 공식 문서](https://spacy.io/usage/linguistic-features#pos-tagging)에서 몇몇 `token.pos_`의 예시를 소개해줍니다.\n",
        "- 지원하는 모든 품사 태그는 [공식 리포의 이 부분](https://github.com/explosion/spaCy/blob/cc5aeaed29c067f60d11e07496704406a1577a35/spacy/glossary.py#L17-L97)에서 확인 가능합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hotveUNLBOE",
        "outputId": "a1f859be-e07b-4e96-c046-96add87b1318"
      },
      "source": [
        "print(spacy.explain(\"PRP\"))  #  e.g. you, them, I\n",
        "print(spacy.explain(\"PRP$\"))  # e.g. your, their, my\n",
        "print(spacy.explain(\"AUX\")) # e.g. auxilary = 보\"조\"하는 이라는 뜻의 형용사입니다. 즉 AUX는 \"조\"동사를 의미합니다: do, can, will \n",
        "print(spacy.explain(\"SCONJ\")) # e.g. if, because, when \n",
        "print(spacy.explain(\"DET\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pronoun, personal\n",
            "pronoun, possessive\n",
            "auxiliary\n",
            "subordinating conjunction\n",
            "determiner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G--ANUbqKJkd"
      },
      "source": [
        "그런 품사 태그를 활용하면 관용구를 검출할 수 있는 패턴을 정의할 수 있습니다. 어떻게 할 수 있을까요? *stand on **one's** own feet*, *open **one's** eyes* 모두 ***one's*** 라는, 인칭 대명사의 소유격 형태를 요구합니다 (e.g. my, her, him, their). 그렇다면, 각 관용구의 검출 패턴을 다음과 같이 품사(POS)로 정의하면, `one's`의 여러 변화형에 대응할 수 있을 것입니다:\n",
        " - `stand on [POS=인칭소유격] own feet`\n",
        " - `open [POS=인칭소유격] eyes`\n",
        " \n",
        "이를 위해선 `one's`라는 토큰이 위치하는 자리에 해당하는 패턴을 만들어줘야 합니다.\n",
        "\n",
        "우선, `nlp`로 각 관용구를 토크나이즈를 했을 때, 바라는대로 `one's`라는 토큰이 출력되는지 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va1sM5JPJYh7",
        "outputId": "ec56fc7a-2b23-445f-f948-04e8bde476f3"
      },
      "source": [
        "for idiom in IDIOMS: \n",
        "  print(list(nlp(idiom)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[stand, on, one, 's, own, feet]\n",
            "[open, one, 's, eyes]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAlo51jXLqHi"
      },
      "source": [
        "문제가 보입니다. `one's`가 하나의 토큰으로 존재하지 않고, `one` , `'s`로 나뉘어집니다. \n",
        "\n",
        "두 토큰으로 나누지 않고 `one's`를 하나의 토큰으로 취급하기 위해,  [`tokenizer.add_special_case`](https://spacy.io/api/tokenizer#add_special_case) 함수를 사용하여 다음과 같이 \n",
        "새로운 토큰화 규칙을 추가해줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_NYmDk9Q8iQ",
        "outputId": "804f46eb-8431-484b-8cec-1d64be685d01"
      },
      "source": [
        "SPECIAL_CASE = [{\"ORTH\": \"one's\"}]\n",
        "nlp.tokenizer.add_special_case(PRP_PLACEHOLDER, SPECIAL_CASE)\n",
        "for idiom in IDIOMS: \n",
        "  # 이제 one's를 하나의 토큰으로 취급할 것 입니다.\n",
        "  print(list(nlp(idiom)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[stand, on, one's, own, feet]\n",
            "[open, one's, eyes]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3CHhMd-Ly6y",
        "outputId": "65eebca3-1f2a-4910-c8a1-96525cc62843"
      },
      "source": [
        "# Q: \"NORM\" 은 무슨 뜻인가요?\n",
        "# A: 말그대로 단어의 정규화된(NORMalized) 폼을 말합니다. 표제어랑 거의 비슷한 용도로 사용되나,\n",
        "# 표제어 != 정규화된 폼인 경우가 있는 경우, NORM이라는 것을 따로 정의해야합니다.\n",
        "# 예를 들어, going to 의 줄임말인 gonna는 gon, na 로 쪼개야 합니다. 여기서 \"gon\"의 표제어는\n",
        "# \"go\"이지만, 정규화된 폼은 \"going\"입니다. 표제어와 정규화된 폼이 다릅니다.\n",
        "# 참고: https://stackoverflow.com/a/46378113\n",
        "case = [{\"ORTH\": \"gon\", \"LEMMA\": \"go\", \"NORM\": \"going\"}, {\"ORTH\": \"na\", \"LEMMA\": \"to\"}]\n",
        "nlp.tokenizer.add_special_case(\"gonna\", case)\n",
        "tokens = [\n",
        "          (token.text, token.lemma_, token.norm_)\n",
        "          for token in nlp(\"I'm gonna do it\")\n",
        "]\n",
        "print(tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', '-PRON-', 'i'), (\"'m\", 'be', 'am'), ('gon', 'go', 'going'), ('na', 'to', 'na'), ('do', 'do', 'do'), ('it', '-PRON-', 'it')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ_54z1lR8HN"
      },
      "source": [
        "**Question: spacy의 `Matcher`로는 무엇을 할 수 있나요?**\n",
        "> Answer: 정규표현식과 비슷한 기능을 합니다. 다만 Matcher로는 표제어(LEMMA), 품사(POS)로 패턴을 정의하여 보다 더 정교한 패턴을 편리하게 정의할 수 있습니다. \n",
        "- spacy의 `Matcher`에 대한 더 자세한 내용은 [이 문서](https://spacy.io/api/matcher)에서 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VHUODiw8elC",
        "outputId": "20d2e477-21e8-4c7b-8128-6978d4866c41"
      },
      "source": [
        "# --- Matcher 의 사용예시 --- # \n",
        "# 문법적 오류가 없는 문장, 그리고 의미상 호감을 표현하는 문장만 매치를 해봅시다.\n",
        "KEY = \"a grammatically correct expression of affection\"\n",
        "EG_BATCH = [\n",
        "            \"I love apples\",\n",
        "            \"I loved him\", \n",
        "            \"I liked her\",\n",
        "            \"I liked\",\n",
        "            \"I loved\",\n",
        "            \"I hate them\"\n",
        "]\n",
        "\n",
        "# 이를 위해 다음과 같은 패턴을 만들어볼 수 있습니다.\n",
        "patterns = [\n",
        "  {\"TEXT\": \"I\"},  # 첫번째 단어는 반드시 I로 하겠다.\n",
        "  {\"LEMMA\": {\"IN\": [\"like\", \"love\"]}},  # 두번째 단어의 표제어는 반드시 like, love 중 하나여야 한다.\n",
        "  {\"POS\": {\"IN\": [\"PRON\", \"NOUN\"]}}  # 품사추출의 활용; liked, loved는 타동사 이므로, 세번째 단어로는 반드시 목적어가 와야한다.\n",
        "]\n",
        "\n",
        "# vocab을 기억한다. key에 새로운 정수를 부여하기 위해서 nlp.vocab을 인자로 받는다. \n",
        "love_matcher = Matcher(nlp.vocab)\n",
        "love_matcher.add(KEY, [patterns]) # 다음의 문서참조; https://spacy.io/api/matcher#add\n",
        "for sent in EG_BATCH:\n",
        "  doc = nlp(sent)  # Doc\n",
        "  matches = love_matcher(doc)  # 객체 자체를 함수로 쓰는 건 뭐지?\n",
        "  if matches:\n",
        "    match_id, start_idx, end_idx = matches[0] \n",
        "    print(match_id)\n",
        "    # nlp.vocab.strings를 통해, 토큰의 id -> 토큰의 str을 얻을 수 있습니다.\n",
        "    key = nlp.vocab.strings[match_id]\n",
        "    # 결과를 확인해볼게요!\n",
        "    print(doc[start_idx: end_idx], \"->\", key)\n",
        "# ---------------------- #\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10756009303937775286\n",
            "I love apples -> a grammatically correct expression of affection\n",
            "10756009303937775286\n",
            "I loved him -> a grammatically correct expression of affection\n",
            "10756009303937775286\n",
            "I liked her -> a grammatically correct expression of affection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_LP7rBTvbL"
      },
      "source": [
        "자, 그럼 이제 본격적으로 `IDIOMS`를 검출할 수 있는 `patterns`를 만들어보겠습니다. \n",
        "\n",
        "앞서, 각 관용구를 검출하기 위해선 다음과 같이 패턴을 정의할 수 있을 것이라고 했습니다:\n",
        " - `stand on [POS=인칭소유격] own feet`\n",
        " - `open [POS=인칭소유격] eyes`\n",
        " \n",
        "아울러, 우리는 동사의 활용형에도 대응을 해야합니다. stand는 standing으로,\n",
        " open은 opened로 얼마든지 변형될 수 있습니다. 이 부분은 어떻게 대응할 수 있을까요? \n",
        " - 힌트: 위 `love_matcher`는 어떻게 `liked`, `loved`에 대응할 수 있었나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeNr1gICEOJ_"
      },
      "source": [
        "def build_patterns(idiom: str) -> List[dict]:\n",
        "    # PRP_PLACEHOLDER & nlp는 local variable이 아닌 global variable (전역변수)입니다.\n",
        "    # 그 점을 상기하기 위해, 그리고 함수내에서 동명의 변수를 만들지 않기 위해 global선언을 해줍시다.\n",
        "    global PRP_PLACEHOLDER, nlp\n",
        "    # 각 패턴은 dictionary 객체로 정의됩니다. 모든 패턴을 리스트로 모아주는 것이 목표입니다.\n",
        "    patterns: List[dict] = list()\n",
        "    doc = nlp(idiom)\n",
        "    # --- TODO 1 --- #\n",
        "    # hint1: 만약 token.text == one's 라면, 패턴을 어떻게 정의해야할까요?\n",
        "    # hint2: 나머지 단어는 패턴을 어떻게 정의해야할까요? LEMMA를 활용해볼 수 있을까요?\n",
        "    for token in doc:\n",
        "      if token.text == PRP_PLACEHOLDER:\n",
        "        # text == 'one's' 패턴을 어떻게 정의?\n",
        "        pattern = {\"TAG\": \"PRP$\"}\n",
        "      else:\n",
        "        pattern = {\"LEMMA\": token.lemma_}\n",
        "      patterns.append(pattern)\n",
        "\n",
        "    return patterns\n",
        "    # -------------- #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDn5ZS89G0DE",
        "outputId": "648f4ec4-cebd-4e56-d785-0a24d6977657"
      },
      "source": [
        "build_patterns(\"stand on one's own feet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'LEMMA': 'stand'},\n",
              " {'LEMMA': 'on'},\n",
              " {'TAG': 'PRP$'},\n",
              " {'LEMMA': 'own'},\n",
              " {'LEMMA': 'foot'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q2w6GA8LImQ"
      },
      "source": [
        "\n",
        "def build_patterns_list_comp(idiom: str) -> List[dict]:\n",
        "  global PRP_PLACEHOLDER, nlp\n",
        "  # --- TODO 2 --- # \n",
        "  # list comprehension을 활용하면, build_patterns와 동일한 코드를 한줄로(!?)작성할 수 있습니다.\n",
        "  # 한번 도전해보시길! 못해도 괜찮습니다. 수업 때 같이 생각해봐요!\n",
        "  # hint: if-else 문을 list comprehension 속에 집어넣을 수도 있습니다! - https://stackoverflow.com/a/4406399\n",
        "  patterns: List[dict] = [\n",
        "                          {\"TAG\": \"PRP$\"} if token.text == PRP_PLACEHOLDER\n",
        "                          else {\"LEMMA\": token.lemma_}\n",
        "                          for token in nlp(idiom)\n",
        "  ]  # pythonic. \n",
        "  return patterns\n",
        "  # -------------- #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4KxgbPs5CEe"
      },
      "source": [
        "# 이제 여러분이 구현한 함수를 바탕으로, 관용구를 검출할 수 있는 matcher를 만들어보겠습니다!\n",
        "idiom_matcher = Matcher(nlp.vocab)\n",
        "for idiom in IDIOMS:\n",
        "  # idiom_patterns = build_patterns_list_comp(idiom)\n",
        "  # list comprehension으로 구현하는 것에 성공하셨다면, 아래 함수를 사용해서 테스트해보세요!\n",
        "  idiom_patterns = build_patterns(idiom)\n",
        "  idiom_matcher.add(idiom, [idiom_patterns])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47uQDPqPbi50"
      },
      "source": [
        "**Question: 객체 자체를 함수로 쓰는 것은 뭐죠? (e.g. `love_matcher(doc)`, `idiom_matcher(doc` )**\n",
        "> A: 객체 자체를 호출 할 경우, 해당 객체의 `__call__()` 구현체가 실행이됩니다.\n",
        "- 즉 [`matcher.__call__()`](https://spacy.io/api/matcher) 이 실행됩니다.\n",
        "- 애초에 클래스 이름이 `Matcher`이므로, 구태여 `matcher.match()`를 하지 않기 위해 `__call__()` dunder method를 구현한 것으로 예상됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT898pW0b_lW",
        "outputId": "d0c25fa1-ab31-4087-f3dd-5a02753936de"
      },
      "source": [
        "# ---  object.__call__() dunder method 의 사용 예시 --- #\n",
        "class Printer:\n",
        "  def __call__(self, name: str) -> str:\n",
        "    return \"{} called __call__\".format(name)\n",
        "\n",
        "  def print(self, name: str):\n",
        "    return \"{} called __call__\".format(name)\n",
        "\n",
        "printer = Printer()\n",
        "# 물론 이렇게 할수도 있지만... 애초에 클래스 이름이 Printer인데, 구태여 print라는 함수를 또 만들기에는 이름이 중복됩니다:\n",
        "print(printer.print(\"유빈이\"))  \n",
        "# 그럴바에는 그냥 printer를 호출했을 때 \"print\"를 하는 것이 더 간결하겠죠: \n",
        "print(printer(\"베프는\")) \n",
        "# 그리고 말씀드린대로, 객체를 호출하는 것은 결국 __call__()을 호출하는 것과 같습니다:\n",
        "print(printer.__call__(\"석신이\"))\n",
        "#--------------------------------------------------- #\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "유빈이 called __call__\n",
            "베프는 called __call__\n",
            "석신이 called __call__\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7QpSEK_alGX",
        "outputId": "cb1b0641-9197-41e2-948a-bc0e3ed72be7"
      },
      "source": [
        "# 이제 준비한 BATCH로 idiom_matcher를 테스트 해봅시다. \n",
        "correct = 0\n",
        "total = len(BATCH)\n",
        "for eg, idiom in BATCH:\n",
        "  doc = nlp(eg)\n",
        "  res = idiom_matcher(doc)\n",
        "  if res:  # if len(res) > 0와 의미는 같습니다.\n",
        "    # 한 문장에 여러 관용구가 나올수도 있으므로, res의 타입은 list입니다.\n",
        "    # 우리의 BATCH 데이터에는 한 문장에 하나의 관용구만 존재하므로, res[0]로 첫번째 결과를 가져와줍니다.\n",
        "    match_id, start_idx, end_idx = res[0]\n",
        "    match_idiom = nlp.vocab.strings[match_id]\n",
        "    # list slicing으로 해당 관용구의 활용형을 가져옵니다.\n",
        "    print(doc[start_idx:end_idx], \"->\", match_idiom)\n",
        "    if match_idiom == idiom:\n",
        "      correct += 1\n",
        "\n",
        "# 최대값인 1.0이 나오면 성공입니다!\n",
        "print(\"관용구 검출 정확도:\", correct / total)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stand on your own feet -> stand on one's own feet\n",
            "stand on their own feet -> stand on one's own feet\n",
            "standing on my own feet -> stand on one's own feet\n",
            "opened my eyes -> open one's eyes\n",
            "opened her eyes -> open one's eyes\n",
            "관용구 검출 정확도: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR8VycdRZ8TF"
      },
      "source": [
        "다음과 같은 결과가 나오면 됩니다:\n",
        "```\n",
        "stand on your own feet -> stand on one's own feet\n",
        "stand on their own feet -> stand on one's own feet\n",
        "standing on my own feet -> stand on one's own feet\n",
        "opened my eyes -> open one's eyes\n",
        "opened her eyes -> open one's eyes\n",
        "관용구 검출 정확도: 1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0kYqPrvauOS"
      },
      "source": [
        "---\n",
        "모두 수고하셨습니다 :) 오타 정정 및 질문은 DM으로 보내주세요!\n"
      ]
    }
  ]
}