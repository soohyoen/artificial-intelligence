{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2_A_cluster_words_class_1.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soohyoen/artificial-intelligence/blob/main/Copy_of_2_A_cluster_words_class_1_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIKsUuNbRQt7"
      },
      "source": [
        "# Week 2-A 실습: Clustering Words with Co-occurence matrix\n",
        "- author: Eu-Bin KIM\n",
        "- @likelion\n",
        "- tlrndk123@gmail.com\n",
        "- 9th of August 2021\n",
        "\n",
        "## Overview\n",
        "### `eval_clusters` 는 뭘하는 함수?\n",
        "이 실습은 `target_words` 와 `target_words_reversed` 를 클러스터링 하는 것에 목표를 둡니다.\n",
        "예를 들어,\n",
        "```python3\n",
        "target_words = [\"abstraction\", \"actually\", \"add\"]\n",
        "target_words_reversed = [\"noitcartsba\", \"yllautca\", \"dda\"]\n",
        "```\n",
        "이라고 한다면, 먼저 이 단어들의 벡터 표현을 말뭉치로부터 co-occurence matrix 를 구축하여  얻습니다. 이 6개의 벡터포현을 클러스터링을 했을 때, 반전된 단어가 반전되기전 단어와 같은 클러스터에 존재하면 co-occurence matrix의 퀄리티가 좋다고 볼 수 있습니다:\n",
        "```\n",
        "# comat의 성능이 매우 좋음: accuracy = 3 / 3 = 1\n",
        "[[\"abstraction\",\"noitcartsba\"],  \n",
        " [\"actually\", \"yllautca\"],\n",
        " [\"add\", \"dda\"]]\n",
        "\n",
        "# comat의 성능이 매우 안 좋음 accuracy = 0 / 3 = 0\n",
        "[[\"abstraction\",\"yllautca\"],  \n",
        " [\"actually\", \"noitcartsba\", \"dda\"],\n",
        " [\"add\"]]\n",
        "```\n",
        "그렇게 pseudo-evaluation을 진행하면, 현재 구한 co-occurence matrix의 퀄리티를 정량적으로\n",
        "측정할 수 있게됩니다(`accuracy`로 측정). 이를 측정하는 함수가 `eval_clusters` 함수입니다.  \n",
        "\n",
        "### `reverse_half` 는 뭘하는 함수?\n",
        "\n",
        "이런식의 평가를 진행하기 위해선, 반전된 단어의 벡터표현을 얻어야 합니다. 하지만 말뭉치에는\n",
        "반전된 단어가 존재하지 않습니다. 때문에 말뭉치에 존재하는 `target_words`의 절반을 반전하는 작업이 필요합니다. 이를 위한 함수가 `reverse_half`입니다.\n",
        "\n",
        "예를들어,\n",
        "```python3\n",
        "CORPUS = \"actually, I actually like the way you actually speak. you actually seem to be a nice person.\"\n",
        "```\n",
        "이런 작은 말뭉치가 있다면, `CORPUS`에서 나타나는 `actually`의 절반을 뒤집어서 `yllautca`로 바꿔줍니다:\n",
        "```python3\n",
        "CORPUS = \"actually, I yllautca like the way you actually speak. you yllautca seem to be a nice person.\"\n",
        "```\n",
        "\n",
        "이 전처리된 말뭉치를 바탕으로 co-occurence matrix를 구축하면, 이제, `actually`와 `yllautca`의 벡터표현을 모두 얻을 수 있게됩니다. 퀄리티가 좋은 co-occurence matrix라면 두 벡터는 매우 유사할 것입니다 (e.g. 모두 `like`, `speak`같은 동사와 같이 출현함). \n",
        "\n",
        "\n",
        "### `build_count_comat`은 무엇을 하는 함수?\n",
        "\n",
        "그렇다면 위에서 언급한 co-occurence matrix란 무엇일까요? 우리가 알고 있는 Document-Term Matrix와 크게 다른 점은 없고, 단지 Document 와 Term 모두 말뭉치에서 추출한 어휘로 설정하게 되면, co-occurence matrix라고 부릅니다. \n",
        "\n",
        "예를들어, 다음과 같은 문장이 있다고 생각해보겠습니다:\n",
        "> Roses are red. bees are yellow.\n",
        "\n",
        "만약 window가 2라면, 이 문장을 바탕으로 다음과 같은 bigrams를 만들어줄 수 있습니다.:\n",
        "```\n",
        "windows = [(Roses, are), (are, red), (red, bees), (bees, are), (are, yellow)]\n",
        "```\n",
        "\n",
        "그럼 각 window를 살펴보며 다음과 같은 co-occurence matrix를 구축할 수 있습니다:\n",
        "```\n",
        "      |  Roses | are | red | bees | yellow\n",
        "Roses |    1   |  1  |  0  |  0   |    0\n",
        "are   |    1   |  1  |  1  |  1   |    1\n",
        "red   |    1   |  1  |  1  |  1   |    0\n",
        "bees  |    0   |  1  |  1  |  1   |    1\n",
        "yellow|    0   |  0  |  0  |  0   |    1\n",
        "```\n",
        "\n",
        "Roses와 are이 \"co-occur\"하는 window는 하나 뿐이기에 `comat[0, 1]` 에 해당하는 값은 1이 됩니다. are은 존재하는 모든 어휘와 co-occur하는 단어 이기에, `comat[1, :]`은 전부 1이 됩니다.\n",
        "이렇게 주어진 `windows`를 바탕으로 `comat`을 구축하는 함수가 `build_count_comat`함수 입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY60NBqUHP_N",
        "outputId": "cfaa0047-04ef-46ec-dfe0-fbd7ba586c00"
      },
      "source": [
        "!pip3 install nltk\n",
        "!pip3 install scikit-learn\n",
        "from typing import List, Optional, Tuple\n",
        "import nltk\n",
        "from nltk.corpus import brown, product_reviews_2, stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "# --- constants --- #\n",
        "# the target words to perform clustering on (total of 50 words)\n",
        "TARGET_WORDS: str = \\\n",
        "\"\"\"abstraction\n",
        "actually\n",
        "add\n",
        "address\n",
        "answer\n",
        "argument\n",
        "arguments\n",
        "back\n",
        "call\n",
        "car\n",
        "case\n",
        "cdr\n",
        "computer\n",
        "course\n",
        "dictionary\n",
        "different\n",
        "evaluator\n",
        "function\n",
        "general\n",
        "got\n",
        "idea\n",
        "kind\n",
        "lambda\n",
        "machine\n",
        "mean\n",
        "object\n",
        "operator\n",
        "order\n",
        "pair\n",
        "part\n",
        "particular\n",
        "pattern\n",
        "place\n",
        "problem\n",
        "process\n",
        "product\n",
        "program\n",
        "reason\n",
        "register\n",
        "result\n",
        "set\n",
        "simple\n",
        "structure\n",
        "system\n",
        "they\n",
        "together\n",
        "using\n",
        "variable\n",
        "why\n",
        "zero\"\"\"\n",
        "\n",
        "BROWN_NAME = \"brown\"\n",
        "PR2_NAME = \"product_reviews_2\"\n",
        "RAND_STATE = 318  # to be used for k-means clustering\n",
        "random.seed(RAND_STATE)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('wordnet')  # for lemmatisation.\n",
        "nltk.download('stopwords') # for stopwords filtering\n",
        "nltk.download(BROWN_NAME)\n",
        "nltk.download(PR2_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package product_reviews_2 to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/product_reviews_2.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyPE2r0IAMh4",
        "outputId": "f2e31bea-9a00-4868-bbb0-2de57dde6b65"
      },
      "source": [
        "# brown corpus의 첫 10개의 단어\n",
        "print(list(brown.words())[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWOkCSvuHQeJ"
      },
      "source": [
        "def reverse_half(words: List[str], targets: List[str]):\n",
        "    \"\"\"\n",
        "    reverse half of the occurences of target words in `words` (in-place)\n",
        "    :param words:\n",
        "    :param targets\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    ### TODO 1 ###\n",
        "    # 말뭉치에 나타나는 target words의 절반을 뒤집어준다.\n",
        "    # car -> rac.\n",
        "    # use random.sample()\n",
        "    occ_idxs = ...\n",
        "    sub_idxs = ...\n",
        "    for idx in sub_idxs:\n",
        "        words[idx] = \"\".join(reversed(words[idx]))\n",
        "    ##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZYPkMgs1Dor"
      },
      "source": [
        "- 예시:\n",
        "- 입력: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYIsfu5nHdFt"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, words: List[str]):\n",
        "        # 인덱스가 주어지면, 인덱스에 해당하는 어휘를 반환\n",
        "        self.idx2word = list(set(words))\n",
        "        # 어휘가 주어지면, 어휘에 해당하는 인덱스를 반환\n",
        "        self.word2idx = {\n",
        "            word: idx\n",
        "            for idx, word in enumerate(self.idx2word)\n",
        "        }\n",
        "\n",
        "    def __contains__(self, item: str):\n",
        "        return item in self.idx2word\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9J5gOuLHfKI"
      },
      "source": [
        "# stemmer, lemmatiser는 제가 구현했습니다 :)\n",
        "def stem(words: List[str]):\n",
        "    global stemmer\n",
        "    for idx, word in enumerate(words):\n",
        "        words[idx] = stemmer.stem(word)\n",
        "\n",
        "def lemmatise(words: List[str]):\n",
        "    global lemmatiser\n",
        "    for idx, word in enumerate(words):\n",
        "        words[idx] = lemmatiser.lemmatize(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcADWzm-Hh9Y"
      },
      "source": [
        "def build_count_comat(vocab: Vocab, windows: List[Tuple[str]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    count the frequencies of the co-occurrences.\n",
        "    # dtm. documents = vocab. terms = vocab.\n",
        "    :param vocab:\n",
        "    :param windows:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    num_words = len(vocab)\n",
        "    comat = np.zeros(shape=(num_words, num_words))\n",
        "    for window in tqdm(windows, desc=\"building count comat...\"):\n",
        "        for term_1 in window:\n",
        "            for term_2 in window:\n",
        "                  ### TODO 2 ###\n",
        "                  # use vocab.word2idx, 파이썬의 try-catch pattern, word in vocab\n",
        "                  # 만약 term_1과 term_2 모두 어휘에 해당하는 단어라면, 해당 comat에서\n",
        "                  # 해당 co-occurence를 +1 한다.\n",
        "                  ##############\n",
        "    # set the diagonal to zero (useless)\n",
        "    comat[range(num_words), range(num_words)] = 0\n",
        "    ###########\n",
        "    return comat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G20GJFZ2Hjts"
      },
      "source": [
        "def cluster_target_words(n_clusters, tfidf_mat: np.array, vocab: Vocab) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    K-means 알고리즘을 활용해서, 클러스터링을 진행합니다.\n",
        "    \"\"\"\n",
        "    clusters: List[List[str]] = [list() for _ in range(n_clusters)]  # a bucket to collect clusters\n",
        "    k_means = KMeans(n_clusters=n_clusters, random_state=RAND_STATE)\n",
        "    result = k_means.fit(tfidf_mat)\n",
        "    for word_idx, cluster_idx in enumerate(result.labels_):\n",
        "        word = vocab.idx2word[word_idx]\n",
        "        # append the word to the cluster\n",
        "        clusters[cluster_idx].append(word)\n",
        "    return clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d40WZcpbHlL4"
      },
      "source": [
        "def eval_clusters(clusters: List[List[str]], targets: List[str], targets_reversed: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    returns the accuracy.\n",
        "    :param clusters:\n",
        "    :param targets\n",
        "    :param targets_reversed\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    pairs = list(zip(targets, targets_reversed))\n",
        "    total = len(pairs)\n",
        "    correct = 0\n",
        "    ### TODO 3 ###\n",
        "    # targets (반전하기전), target_reversed (반전 후)가 모두 같은 클러스터에 존재하는지\n",
        "    # 확인해서, 같은 클러스터에 존재한다면 correct += 1.\n",
        "\n",
        "    ##############\n",
        "    return correct / total\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PKwTe5MHnCa"
      },
      "source": [
        "def run_experiment(corpus_name: str,\n",
        "                   lower_case: bool,\n",
        "                   remove_stopwords: bool,\n",
        "                   norm_mode: Optional[str],\n",
        "                   window_size: int):\n",
        "    if corpus_name == BROWN_NAME:\n",
        "        # nltk에서 제공하는 함수\n",
        "        corpus = brown\n",
        "    elif corpus_name == PR2_NAME:\n",
        "        corpus = product_reviews_2\n",
        "    else:\n",
        "        raise ValueError\n",
        "    targets = TARGET_WORDS.split(\"\\n\")\n",
        "    # 말뭉치에 있는 모든 단어 \n",
        "    words: List[str] = list(corpus.words())\n",
        "    # 이후에는 파라미터에 따라 말뭉치를 전처리.\n",
        "    # --- case folding --- #\n",
        "    if lower_case:\n",
        "        words = [word.lower() for word in words]\n",
        "    # --- stopwords filtering --- #\n",
        "    if remove_stopwords:\n",
        "        words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    # --- 정규화: stemming or lemmatisation --- #\n",
        "    if norm_mode == \"stem\":\n",
        "        # then.. you must stem the vocab.\n",
        "        stem(words)  # stem words in-place\n",
        "        stem(targets)\n",
        "    elif norm_mode == \"lemmatise\":\n",
        "        lemmatise(words)  # lemmatise words in-place\n",
        "        lemmatise(targets)\n",
        "    # --- ngrams 으로 맥락 윈도우 구축하기 --- #\n",
        "    reverse_half(words, targets)  # for pseudo-eval\n",
        "    windows = list(ngrams(words, window_size))\n",
        "    # --- 어휘 구축하기 --- #\n",
        "    targets_reversed = [\"\".join(reversed(target)) for target in targets]\n",
        "    vocab = Vocab(words=targets + targets_reversed)\n",
        "    # --- build a word2word co-occurrence matrix (dtm), where both documents & terms are target words --- #\n",
        "    comat = build_count_comat(vocab, windows)\n",
        "    # --- cluster the target words using their tfidf vectors --- #\n",
        "    n_clusters = len(vocab) // 2\n",
        "    clusters = cluster_target_words(n_clusters, comat, vocab)\n",
        "    # --- evaluate the clusters;check if the reversed form is included in the same cluster --- #\n",
        "    acc = eval_clusters(clusters, targets, targets_reversed)\n",
        "    # --- 결과 리포트 --- #\n",
        "    print(\"### REPORT ###\")\n",
        "    print(\"vocab_size:{}, n_clusters:{}, lower_case: {}, remove_stopwords:{}, corpus_name:{}, stem_or_lemmatise:{}, window_size:{}\"\n",
        "          .format(len(vocab), n_clusters, lower_case, remove_stopwords, corpus_name, norm_mode, window_size))\n",
        "    pprinter = PrettyPrinter(compact=True)\n",
        "    pprinter.pprint(clusters)\n",
        "    print(\"accuracy:\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID_p6p-nHr9j"
      },
      "source": [
        "  \n",
        "  run_experiment(corpus_name=BROWN_NAME, lower_case=False, remove_stopwords=False, norm_mode=None,  window_size=8)\n",
        "  # the effect of lemmatisation on the performance\n",
        "  run_experiment(corpus_name=BROWN_NAME, lower_case=False, remove_stopwords=False, norm_mode=\"lemmatise\", window_size=8)\n",
        "  # the effect of stemming on the performance\n",
        "  run_experiment(corpus_name=BROWN_NAME, lower_case=False, remove_stopwords=False,  norm_mode=\"stem\", window_size=8)  # 이게 아마도 성능이 제일 좋을 겁니다.\n",
        "  # the effect of case folding on the performance\n",
        "  run_experiment(corpus_name=BROWN_NAME,  lower_case=True, remove_stopwords=False, norm_mode=\"stem\", window_size=8)\n",
        "  # the effect of switching corpus on the performance\n",
        "  run_experiment(corpus_name=PR2_NAME,  lower_case=False, remove_stopwords=False, norm_mode=\"stem\", window_size=8)\n",
        "  # the effect of removing stopwords\n",
        "  run_experiment(corpus_name=BROWN_NAME,  lower_case=False, remove_stopwords=True, norm_mode=\"stem\", window_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_IOES4-iB_u"
      },
      "source": [
        "다음과 같은 결과가 나와야 합니다:\n",
        "\n",
        "(결과가 다를수도 있습니다) -> \n",
        "```\n",
        "building count comat...: 100%|██████████| 1161185/1161185 [00:09<00:00, 119558.38it/s]\n",
        "### REPORT ###\n",
        "vocab_size:100, n_clusters:50, lower_case: False, remove_stopwords:False, corpus_name:brown, stem_or_lemmatise:None, window_size:8\n",
        "[['redro'], ['yeht'], ['they'], ['program', 'yllautca'], ['rehtegot'],\n",
        " ['rotarepo', 'elpmis'], ['yhw'], ['reason'], ['llac'], ['got'], ['kcab'],\n",
        " ['tnereffid'], ['esruoc'], ['tes'], ['metsys'], ['different'], ['margorp'],\n",
        " ['back'], ['system'], ['course'], ['rac'], ['why'], ['kind'], ['nosaer'],\n",
        " ['together'], ['place'], ['tog'], ['ecalp'], ['ralucitrap'], ['order'],\n",
        " ['part'], ['aedi'],\n",
        " ['machine', 'retsiger', 'operator', 'computer', 'stnemugra', 'enihcam',\n",
        "  'tcejbo', 'dda', 'arguments', 'variable', 'cdr', 'sserdda', 'address',\n",
        "  'tcudorp', 'gnisu', 'noitcartsba', 'rdc', 'pair', 'abstraction', 'evaluator',\n",
        "  'register', 'elbairav', 'retupmoc', 'yranoitcid', 'dictionary', 'zero',\n",
        "  'structure', 'erutcurts', 'rotaulave', 'adbmal', 'nrettap', 'orez',\n",
        "  'function', 'product', 'result', 'lambda', 'noitcnuf', 'riap', 'melborp',\n",
        "  'argument'],\n",
        " ['set'], ['dnik'], ['object'], ['ssecorp'], ['trap'], ['problem'], ['process'],\n",
        " ['tluser'], ['lareneg'], ['call', 'actually', 'esac', 'add'], ['car'],\n",
        " ['using'], ['particular'],\n",
        " ['naem', 'case', 'pattern', 'tnemugra', 'answer', 'general', 'idea'],\n",
        " ['simple'], ['mean'], ['rewsna']]\n",
        "accuracy: 0.32\n",
        "building count comat...: 100%|██████████| 1161185/1161185 [00:09<00:00, 120675.27it/s]\n",
        "### REPORT ###\n",
        "vocab_size:98, n_clusters:49, lower_case: False, remove_stopwords:False, corpus_name:brown, stem_or_lemmatise:lemmatise, window_size:8\n",
        "[['rac'], ['yeht'],\n",
        " ['machine', 'rotarepo', 'retsiger', 'operator', 'computer', 'enihcam', 'dda',\n",
        "  'simple', 'pattern', 'actually', 'variable', 'cdr', 'sserdda', 'address',\n",
        "  'tcudorp', 'gnisu', 'noitcartsba', 'rdc', 'pair', 'abstraction', 'evaluator',\n",
        "  'object', 'register', 'using', 'elbairav', 'retupmoc', 'yranoitcid',\n",
        "  'dictionary', 'tnemugra', 'zero', 'structure', 'erutcurts', 'rotaulave',\n",
        "  'adbmal', 'orez', 'function', 'product', 'lambda', 'noitcnuf', 'riap',\n",
        "  'argument'],\n",
        " ['they'], ['case'], ['why'], ['tes'], ['esruoc'], ['got'], ['reason'],\n",
        " ['back'], ['kcab'], ['car'], ['tog'], ['aedi'],\n",
        " ['call', 'tcejbo', 'redro', 'yllautca'], ['rehtegot', 'together'], ['system'],\n",
        " ['different'], ['nosaer'], ['tnereffid'], ['melborp'], ['order'], ['metsys'],\n",
        " ['kind'], ['problem'], ['set'], ['idea'], ['place'],\n",
        " ['answer', 'rewsna', 'tluser'], ['ecalp'], ['part'], ['margorp'], ['process'],\n",
        " ['yhw'], ['mean'], ['dnik'], ['naem'], ['ralucitrap'], ['elpmis'],\n",
        " ['esac', 'particular', 'add', 'llac'], ['trap'], ['lareneg'], ['ssecorp'],\n",
        " ['result'], ['program'], ['nrettap'], ['course'], ['general']]\n",
        "accuracy: 0.42\n",
        "building count comat...: 100%|██████████| 1161185/1161185 [00:09<00:00, 120763.07it/s]\n",
        "### REPORT ###\n",
        "vocab_size:98, n_clusters:49, lower_case: False, remove_stopwords:False, corpus_name:brown, stem_or_lemmatise:stem, window_size:8\n",
        "[['tes'], ['yeht'], ['they'], ['ulave', 'dda', 'answer', 'ralucitrap', 'lpmis'],\n",
        " ['reason'], ['esu'], ['tog', 'htegot'], ['sruoc', 'dnik'], ['whi'],\n",
        " ['set', 'idea'], ['reffid'],\n",
        " ['case', 'pattern', 'comput', 'tnemugra', 'structur', 'nrettap', 'evalu',\n",
        "  'simpl', 'rac', 'tupmoc'],\n",
        " ['use'], ['result', 'tluser'], ['program', 'trap'], ['gener'], ['naem'],\n",
        " ['reneg'], ['problem'], ['call'], ['differ'], ['system'], ['kcab'], ['ihw'],\n",
        " ['redro', 'esac', 'particular', 'rewsna', 'actual'], ['metsys'], ['oper'],\n",
        " ['back'], ['process', 'product'], ['object'], ['function', 'noitcnuf'],\n",
        " ['ssecorp'], ['place'], ['cours', 'lautca'], ['part'], ['aedi'],\n",
        " ['got', 'togeth', 'llac'], ['car'], ['mean'], ['tcejbo'], ['margorp'],\n",
        " ['ecalp'], ['kind'], ['repo'],\n",
        " ['machin', 'tsiger', 'regist', 'variabl', 'cdr', 'sserdda', 'abstract',\n",
        "  'address', 'rdc', 'pair', 'tcartsba', 'iranoitcid', 'rutcurts', 'nihcam',\n",
        "  'zero', 'adbmal', 'add', 'orez', 'lbairav', 'lambda', 'riap', 'argument',\n",
        "  'dictionari'],\n",
        " ['melborp'], ['tcudorp'], ['nosaer'], ['order']]\n",
        "accuracy: 0.28\n",
        "building count comat...: 100%|██████████| 1161185/1161185 [00:09<00:00, 120562.30it/s]\n",
        "### REPORT ###\n",
        "vocab_size:98, n_clusters:49, lower_case: True, remove_stopwords:False, corpus_name:brown, stem_or_lemmatise:stem, window_size:8\n",
        "[['machin', 'variabl', 'esac', 'nihcam', 'lbairav', 'simpl'], ['yeht'],\n",
        " ['they'], ['htegot', 'togeth'], ['use'], ['program'], ['case', 'object'],\n",
        " ['nosaer'], ['ihw'], ['aedi', 'ralucitrap', 'lpmis', 'rewsna', 'dnik'],\n",
        " ['gener'], ['mean'], ['got', 'tog'], ['back'],\n",
        " ['pattern', 'particular', 'rutcurts', 'structur', 'idea', 'nrettap', 'result'],\n",
        " ['differ'], ['cours', 'redro'], ['reneg'], ['esu'], ['tluser'],\n",
        " ['tsiger', 'regist', 'ulave', 'dda', 'cdr', 'sserdda', 'abstract', 'address',\n",
        "  'rdc', 'pair', 'tcartsba', 'iranoitcid', 'comput', 'answer', 'tnemugra',\n",
        "  'zero', 'adbmal', 'add', 'evalu', 'orez', 'lambda', 'noitcnuf', 'riap',\n",
        "  'argument', 'rac', 'tupmoc', 'dictionari'],\n",
        " ['melborp'], ['reffid'], ['metsys'], ['kind'], ['whi'], ['set', 'place'],\n",
        " ['oper'], ['part'], ['margorp', 'process'], ['tcudorp'], ['trap'], ['naem'],\n",
        " ['kcab'], ['order'], ['call'], ['reason'], ['ssecorp'],\n",
        " ['sruoc', 'lautca', 'actual'], ['car'], ['repo'], ['product'], ['llac'],\n",
        " ['problem'], ['tcejbo'], ['system'], ['tes'], ['ecalp'], ['function']]\n",
        "accuracy: 0.4\n",
        "building count comat...: 100%|██████████| 84612/84612 [00:00<00:00, 119893.90it/s]\n",
        "### REPORT ###\n",
        "vocab_size:98, n_clusters:49, lower_case: False, remove_stopwords:False, corpus_name:product_reviews_2, stem_or_lemmatise:stem, window_size:8\n",
        "[['simpl'], ['redro', 'process', 'answer'],\n",
        " ['cours', 'machin', 'tsiger', 'regist', 'aedi', 'ulave', 'tcejbo', 'dda',\n",
        "  'pattern', 'cdr', 'abstract', 'address', 'rdc', 'pair', 'tcartsba', 'object',\n",
        "  'particular', 'ecalp', 'htegot', 'iranoitcid', 'rutcurts', 'tnemugra',\n",
        "  'structur', 'zero', 'idea', 'reneg', 'adbmal', 'nrettap', 'evalu', 'orez',\n",
        "  'lbairav', 'result', 'dnik', 'lambda', 'part', 'noitcnuf', 'riap', 'order',\n",
        "  'argument', 'dictionari', 'tluser'],\n",
        " ['esu'], ['use'], ['repo'], ['they'], ['yeht'], ['llac'], ['actual'],\n",
        " ['problem'], ['tes'], ['tcudorp'], ['system'], ['call'], ['melborp'],\n",
        " ['comput'], ['mean', 'case', 'car', 'nihcam', 'function'], ['metsys'], ['set'],\n",
        " ['tupmoc'], ['program'], ['ihw'], ['margorp'], ['kcab'], ['product'], ['got'],\n",
        " ['oper'], ['back'], ['differ'], ['nosaer'], ['whi'], ['lautca'], ['sruoc'],\n",
        " ['tog'], ['kind'], ['add'], ['ssecorp'], ['rac'], ['esac'], ['sserdda'],\n",
        " ['naem', 'togeth', 'trap', 'rewsna'], ['reffid'], ['gener'], ['ralucitrap'],\n",
        " ['lpmis'], ['reason'], ['place'], ['variabl']]\n",
        "accuracy: 0.3\n",
        "building count comat...: 100%|██████████| 727494/727494 [00:06<00:00, 120995.04it/s]\n",
        "### REPORT ###\n",
        "vocab_size:98, n_clusters:49, lower_case: False, remove_stopwords:True, corpus_name:brown, stem_or_lemmatise:stem, window_size:8\n",
        "[['ihw', 'tsiger', 'regist', 'variabl', 'ulave', 'dda', 'cdr', 'sserdda',\n",
        "  'abstract', 'whi', 'rdc', 'pair', 'tcartsba', 'iranoitcid', 'answer',\n",
        "  'tnemugra', 'ralucitrap', 'structur', 'zero', 'adbmal', 'add', 'orez',\n",
        "  'lpmis', 'rewsna', 'lbairav', 'lambda', 'riap', 'simpl', 'argument', 'tupmoc',\n",
        "  'dictionari'],\n",
        " ['reneg'], ['melborp'], ['set'], ['tluser'], ['got', 'car', 'tog'], ['esu'],\n",
        " ['use'], ['metsys'], ['kcab'], ['gener'], ['repo'], ['ecalp'],\n",
        " ['sruoc', 'esac'], ['object'], ['program'], ['reffid'], ['system'],\n",
        " ['ssecorp'], ['mean'], ['kind', 'case', 'reason'], ['llac'], ['differ'],\n",
        " ['trap'], ['process'], ['yeht'], ['result'], ['noitcnuf'], ['dnik'], ['rac'],\n",
        " ['order'], ['product'], ['redro'], ['margorp'], ['tcudorp'], ['naem'],\n",
        " ['problem'], ['tes'], ['part'], ['call'], ['back'], ['they'],\n",
        " ['machin', 'aedi', 'pattern', 'address', 'particular', 'rutcurts', 'comput',\n",
        "  'lautca', 'nihcam', 'nrettap', 'evalu', 'actual', 'nosaer'],\n",
        " ['idea'], ['oper'], ['htegot', 'togeth'], ['cours'], ['tcejbo', 'function'],\n",
        " ['place']]\n",
        "accuracy: 0.38\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNXu7O3wHwH8"
      },
      "source": [
        "## 다음의 질문에 답하세요.\n",
        "\n",
        ">  stemming을 했을 때, 안했을 때 성능의 차이? 이유는?\n",
        "\n",
        ">  case folding을 했을때, 안 했을 때 성능의 차이? 이유는?\n",
        "\n",
        ">  말뭉치가 BROWN 일 때, 아닐 때, 성능의 차이? 이유는?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sXbF36RIgEq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}